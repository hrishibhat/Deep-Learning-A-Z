{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import usual suspects\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5\n",
       "0   8  13  26  35  45  51\n",
       "1   1  15  24  31  34  44\n",
       "2   3   8  29  30  31  49\n",
       "3  21  25  39  50  54  59\n",
       "4  15  19  32  38  47  50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing  the training set\n",
    "pd_training_set = pd.read_csv('Lottery_NY_Lotto_Winning_Numbers__Beginning_2001_without_bonus.csv', header=None)\n",
    "pd_training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Set\n",
    "# now add each column of six rows into next 6 rows of first column\n",
    "two_dim_lotto_array_train = []\n",
    "# take 300 rows less from total for testing and rest use for training\n",
    "# for col in pd_training_set.iloc[:-300,:].values:\n",
    "for col in pd_training_set.iloc[:,:].values:\n",
    "#     print (col)\n",
    "    for row in col:\n",
    "        two_dim_lotto_array_train.append([row])\n",
    "#         print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9768, 1)\n",
      "[[ 8]\n",
      " [13]\n",
      " [26]\n",
      " ..., \n",
      " [17]\n",
      " [26]\n",
      " [55]]\n"
     ]
    }
   ],
   "source": [
    "# convert python list into numpy array\n",
    "training_set_val = np.array(two_dim_lotto_array_train, ndmin=2)\n",
    "\n",
    "print (type(training_set_val))\n",
    "print (training_set_val.ndim)\n",
    "print (training_set_val.shape)\n",
    "print (training_set_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.73333333]\n",
      " [-0.56666667]\n",
      " [-0.13333333]\n",
      " ..., \n",
      " [-0.43333333]\n",
      " [-0.13333333]\n",
      " [ 0.83333333]]\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9768, 1)\n"
     ]
    }
   ],
   "source": [
    "# # Now, let's do normalization\n",
    "\n",
    "# # from sklearn.preprocessing import MinMaxScaler\n",
    "# # sc = MinMaxScaler()\n",
    "# # # Accuracy: 0.0317802805365\n",
    "\n",
    "# from sklearn.preprocessing import MaxAbsScaler\n",
    "# sc = MaxAbsScaler()\n",
    "# #  Accuracy: 0.0293846626395\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "sc = RobustScaler()\n",
    "# Accuracy: 0.0377802805365\n",
    "training_set = sc.fit_transform(training_set_val)\n",
    "print (training_set)\n",
    "print (type(training_set))\n",
    "print (training_set.ndim)\n",
    "print (training_set.shape)\n",
    "\n",
    "# # ALSO TRY STANDARDIZATION INSTEAD OF NORMALIZATION\n",
    "\n",
    "# # from sklearn.preprocessing import StandardScaler\n",
    "# # sc = StandardScaler()\n",
    "\n",
    "# # training_set = sc.fit_transform(training_set_val)\n",
    "# # print (training_set)\n",
    "# # print (type(training_set))\n",
    "# # print (training_set.ndim)\n",
    "# # print (training_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (9762, 6)\n",
      "X_train [[-0.73333333 -0.56666667 -0.13333333  0.16666667  0.5         0.7       ]\n",
      " [-0.56666667 -0.13333333  0.16666667  0.5         0.7        -0.96666667]\n",
      " [-0.13333333  0.16666667  0.5         0.7        -0.96666667 -0.5       ]\n",
      " ..., \n",
      " [ 0.66666667  0.76666667  0.9        -0.86666667 -0.76666667 -0.6       ]\n",
      " [ 0.76666667  0.9        -0.86666667 -0.76666667 -0.6        -0.43333333]\n",
      " [ 0.9        -0.86666667 -0.76666667 -0.6        -0.43333333 -0.13333333]]\n",
      "y_train.shape (9762,)\n",
      "y_train [-0.96666667 -0.5        -0.2        ..., -0.43333333 -0.13333333\n",
      "  0.83333333]\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 60 timesteps and t+1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(6, 9768):\n",
    "    X_train.append(training_set[i-6:i, 0])\n",
    "    y_train.append(training_set[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "print (\"X_train.shape\", X_train.shape)\n",
    "print (\"X_train\", X_train)\n",
    "print (\"y_train.shape\", y_train.shape)\n",
    "print (\"y_train\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9762, 6, 1)\n",
      "[[[-0.73333333]\n",
      "  [-0.56666667]\n",
      "  [-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]]\n",
      "\n",
      " [[-0.56666667]\n",
      "  [-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]\n",
      "  [-0.96666667]]\n",
      "\n",
      " [[-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]\n",
      "  [-0.96666667]\n",
      "  [-0.5       ]]\n",
      "\n",
      " ..., \n",
      " [[ 0.66666667]\n",
      "  [ 0.76666667]\n",
      "  [ 0.9       ]\n",
      "  [-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]]\n",
      "\n",
      " [[ 0.76666667]\n",
      "  [ 0.9       ]\n",
      "  [-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]\n",
      "  [-0.43333333]]\n",
      "\n",
      " [[ 0.9       ]\n",
      "  [-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]\n",
      "  [-0.43333333]\n",
      "  [-0.13333333]]]\n"
     ]
    }
   ],
   "source": [
    "# Now, Reshaping for keras before training, as it requires 3 dimenions\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Initializing the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the Keras liabraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", input_shape=(None, 1), return_sequences=True, units=6)`\n",
      "  \n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=6)`\n",
      "  import sys\n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=6)`\n",
      "  \n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=6)`\n",
      "  if __name__ == '__main__':\n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=6)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", units=6)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Create a regressor bassed upon sequential RNN\n",
    "# classifier = Sequential()\n",
    "# classifier.add(LSTM(units=10, activation='sigmoid', input_shape=(None, 1)))\n",
    "# classifier.add(Dense(units=1))\n",
    "regressor = Sequential()\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', input_shape=(None, 1), return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid'))\n",
    "regressor.add(Dense(output_dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let' compile our RNN regressor\n",
    "# classifier.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "regressor.compile(optimizer='rmsprop', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.3296    \n",
      "Epoch 2/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.3219    \n",
      "Epoch 3/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.3218    \n",
      "Epoch 4/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.3219    \n",
      "Epoch 5/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.3219    \n",
      "Epoch 6/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.3219    \n",
      "Epoch 7/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.3219    \n",
      "Epoch 8/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.3218    \n",
      "Epoch 9/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.3217    \n",
      "Epoch 10/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.3209    \n",
      "Epoch 11/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.3138    \n",
      "Epoch 12/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.2840    \n",
      "Epoch 13/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.2196    \n",
      "Epoch 14/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.1467    \n",
      "Epoch 15/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.1173    \n",
      "Epoch 16/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.1033    \n",
      "Epoch 17/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0965    \n",
      "Epoch 18/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0922    \n",
      "Epoch 19/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0891    - ETA: 0s - los\n",
      "Epoch 20/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0865    \n",
      "Epoch 21/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0839    \n",
      "Epoch 22/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0815    \n",
      "Epoch 23/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0795    \n",
      "Epoch 24/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0776    \n",
      "Epoch 25/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0760    \n",
      "Epoch 26/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0747    \n",
      "Epoch 27/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0734    \n",
      "Epoch 28/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0727    \n",
      "Epoch 29/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0719    \n",
      "Epoch 30/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0713    \n",
      "Epoch 31/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0704    \n",
      "Epoch 32/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0701    \n",
      "Epoch 33/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0694    \n",
      "Epoch 34/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0690    \n",
      "Epoch 35/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0686    \n",
      "Epoch 36/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0682    \n",
      "Epoch 37/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0677    \n",
      "Epoch 38/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0673    \n",
      "Epoch 39/200\n",
      "9762/9762 [==============================] - 22s - loss: 0.0669    \n",
      "Epoch 40/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0665    \n",
      "Epoch 41/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0662    \n",
      "Epoch 42/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0660    \n",
      "Epoch 43/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0655    \n",
      "Epoch 44/200\n",
      "9762/9762 [==============================] - 22s - loss: 0.0654    \n",
      "Epoch 45/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0650    \n",
      "Epoch 46/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0648    \n",
      "Epoch 47/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0646    \n",
      "Epoch 48/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0642    \n",
      "Epoch 49/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0640    \n",
      "Epoch 50/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0638    \n",
      "Epoch 51/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0635    \n",
      "Epoch 52/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0633    \n",
      "Epoch 53/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0631    \n",
      "Epoch 54/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0627    \n",
      "Epoch 55/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0625    \n",
      "Epoch 56/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0623    \n",
      "Epoch 57/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0621    \n",
      "Epoch 58/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0618    \n",
      "Epoch 59/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0617    \n",
      "Epoch 60/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0614    \n",
      "Epoch 61/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0613    \n",
      "Epoch 62/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0611    \n",
      "Epoch 63/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0609    \n",
      "Epoch 64/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0607    \n",
      "Epoch 65/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0605    \n",
      "Epoch 66/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0605    \n",
      "Epoch 67/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0601    \n",
      "Epoch 68/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0599    \n",
      "Epoch 69/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0596    \n",
      "Epoch 70/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0595    \n",
      "Epoch 71/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0595    \n",
      "Epoch 72/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0595    \n",
      "Epoch 73/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0590    \n",
      "Epoch 74/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0588    \n",
      "Epoch 75/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0588    \n",
      "Epoch 76/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0585    \n",
      "Epoch 77/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0584    \n",
      "Epoch 78/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0583    \n",
      "Epoch 79/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0580    \n",
      "Epoch 80/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0578    \n",
      "Epoch 81/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0576    \n",
      "Epoch 82/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0576    \n",
      "Epoch 83/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0574    \n",
      "Epoch 84/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0574    \n",
      "Epoch 85/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0571    \n",
      "Epoch 86/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0571    \n",
      "Epoch 87/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0569    \n",
      "Epoch 88/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0568    \n",
      "Epoch 89/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0567    \n",
      "Epoch 90/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0566    \n",
      "Epoch 91/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0565    \n",
      "Epoch 92/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0562    \n",
      "Epoch 93/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0562    \n",
      "Epoch 94/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0560    \n",
      "Epoch 95/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0560    \n",
      "Epoch 96/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0560    \n",
      "Epoch 97/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0558    \n",
      "Epoch 98/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0556    \n",
      "Epoch 99/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0557    \n",
      "Epoch 100/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0556    \n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9762/9762 [==============================] - 21s - loss: 0.0554    \n",
      "Epoch 102/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0553    \n",
      "Epoch 103/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0552    \n",
      "Epoch 104/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0553    \n",
      "Epoch 105/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0552    \n",
      "Epoch 106/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0551    \n",
      "Epoch 107/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0550    \n",
      "Epoch 108/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0548    \n",
      "Epoch 109/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0548    \n",
      "Epoch 110/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0549    \n",
      "Epoch 111/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0549    \n",
      "Epoch 112/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0548    \n",
      "Epoch 113/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0547    \n",
      "Epoch 114/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0546    \n",
      "Epoch 115/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0547    \n",
      "Epoch 116/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0546    \n",
      "Epoch 117/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0545    \n",
      "Epoch 118/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0544    \n",
      "Epoch 119/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0544    \n",
      "Epoch 120/200\n",
      "9762/9762 [==============================] - 22s - loss: 0.0543    \n",
      "Epoch 121/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0543    \n",
      "Epoch 122/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0544    \n",
      "Epoch 123/200\n",
      "9762/9762 [==============================] - 26s - loss: 0.0543    \n",
      "Epoch 124/200\n",
      "9762/9762 [==============================] - 29s - loss: 0.0543    \n",
      "Epoch 125/200\n",
      "9762/9762 [==============================] - 30s - loss: 0.0541    \n",
      "Epoch 126/200\n",
      "9762/9762 [==============================] - 32s - loss: 0.0541    \n",
      "Epoch 127/200\n",
      "9762/9762 [==============================] - 34s - loss: 0.0541    \n",
      "Epoch 128/200\n",
      "9762/9762 [==============================] - 32s - loss: 0.0540    \n",
      "Epoch 129/200\n",
      "9762/9762 [==============================] - 30s - loss: 0.0541    \n",
      "Epoch 130/200\n",
      "9762/9762 [==============================] - 32s - loss: 0.0539    \n",
      "Epoch 131/200\n",
      "9762/9762 [==============================] - 31s - loss: 0.0540    \n",
      "Epoch 132/200\n",
      "9762/9762 [==============================] - 30s - loss: 0.0539    \n",
      "Epoch 133/200\n",
      "9762/9762 [==============================] - 30s - loss: 0.0539    \n",
      "Epoch 134/200\n",
      "9762/9762 [==============================] - 29s - loss: 0.0537    \n",
      "Epoch 135/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0538    \n",
      "Epoch 136/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0537    - \n",
      "Epoch 137/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0538    \n",
      "Epoch 138/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0536    \n",
      "Epoch 139/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0537    \n",
      "Epoch 140/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0537    \n",
      "Epoch 141/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0537    \n",
      "Epoch 142/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0536    - ETA: 0s - l\n",
      "Epoch 143/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0534    \n",
      "Epoch 144/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0536    \n",
      "Epoch 145/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0535    \n",
      "Epoch 146/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0535    \n",
      "Epoch 147/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0535    \n",
      "Epoch 148/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0534    \n",
      "Epoch 149/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0534    \n",
      "Epoch 150/200\n",
      "9762/9762 [==============================] - 24s - loss: 0.0533    \n",
      "Epoch 151/200\n",
      "9762/9762 [==============================] - 22s - loss: 0.0535    \n",
      "Epoch 152/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0534    \n",
      "Epoch 153/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0533    \n",
      "Epoch 154/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0533    \n",
      "Epoch 155/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0533    \n",
      "Epoch 156/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0533    \n",
      "Epoch 157/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0532    \n",
      "Epoch 158/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0532    \n",
      "Epoch 159/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0533    \n",
      "Epoch 160/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0531    \n",
      "Epoch 161/200\n",
      "9762/9762 [==============================] - 21s - loss: 0.0533    \n",
      "Epoch 162/200\n",
      "9762/9762 [==============================] - 20s - loss: 0.0531    \n",
      "Epoch 163/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0531    \n",
      "Epoch 164/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0531    \n",
      "Epoch 165/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0530    \n",
      "Epoch 166/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0530    \n",
      "Epoch 167/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0530    \n",
      "Epoch 168/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0530    \n",
      "Epoch 169/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0530    \n",
      "Epoch 170/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0529    \n",
      "Epoch 171/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0529    \n",
      "Epoch 172/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0528    \n",
      "Epoch 173/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0528    \n",
      "Epoch 174/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0528    \n",
      "Epoch 175/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0528    \n",
      "Epoch 176/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0528    \n",
      "Epoch 177/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0528    \n",
      "Epoch 178/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0528    \n",
      "Epoch 179/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0526    \n",
      "Epoch 180/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0527    \n",
      "Epoch 181/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0525    \n",
      "Epoch 182/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0525    \n",
      "Epoch 183/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0524    \n",
      "Epoch 184/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0523    \n",
      "Epoch 185/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0522    \n",
      "Epoch 186/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0521    \n",
      "Epoch 187/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0521    \n",
      "Epoch 188/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0520    \n",
      "Epoch 189/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0521    \n",
      "Epoch 190/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0520    \n",
      "Epoch 191/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0520    \n",
      "Epoch 192/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0519    \n",
      "Epoch 193/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0518    \n",
      "Epoch 194/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0517    \n",
      "Epoch 195/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0518    \n",
      "Epoch 196/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0517    \n",
      "Epoch 197/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0515    \n",
      "Epoch 198/200\n",
      "9762/9762 [==============================] - 18s - loss: 0.0516    \n",
      "Epoch 199/200\n",
      "9762/9762 [==============================] - 19s - loss: 0.0514    \n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9762/9762 [==============================] - 18s - loss: 0.0514    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125222940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regressor.fit(X_train ,y_train, batch_size=32, epochs=50) with 4 LSTM and time 60 i achieved 0.0219 loss with t = 1\n",
    "\n",
    "regressor.fit(X_train ,y_train, batch_size=36, nb_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor.save('lotto_regressor_5.2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Making Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "from keras.models import load_model\n",
    "model = load_model('lotto_regressor_5.1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2   3   4   5\n",
       "0    8  13  26  35  45  51\n",
       "1    1  15  24  31  34  44\n",
       "2    3   8  29  30  31  49\n",
       "3   21  25  39  50  54  59\n",
       "4   15  19  32  38  47  50\n",
       "5   21  25  27  39  44  56\n",
       "6    3   8  13  21  33  41\n",
       "7    8  14  19  35  42  50\n",
       "8    1   6  23  30  40  51\n",
       "9    5  17  30  35  41  49\n",
       "10   5   8  21  24  50  56\n",
       "11   8  18  34  44  50  54\n",
       "12   3   8  25  31  38  44\n",
       "13   1  17  25  27  28  50\n",
       "14   1   2  32  33  36  39\n",
       "15   7  12  20  34  51  52\n",
       "16   8  18  21  22  26  49\n",
       "17   9  10  20  34  55  56\n",
       "18   6   8  11  17  22  41\n",
       "19  21  27  29  34  41  48"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Importing  the training set\n",
    "pd_testing_set = pd.read_csv('Lottery_NY_Lotto_Winning_Numbers__Beginning_2001_without_bonus.csv', header=None)\n",
    "pd_testing_set.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "# now add each column of six rows into next 6 rows of first column\n",
    "two_dim_lotto_array_test = []\n",
    "# take all rows less from total for testing and rest use for training\n",
    "for col in pd_testing_set.iloc[:,:].values:\n",
    "#     print (col)\n",
    "    for row in col:\n",
    "        two_dim_lotto_array_test.append([row])\n",
    "#         print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9768, 1)\n",
      "[[ 8]\n",
      " [13]\n",
      " [26]\n",
      " ..., \n",
      " [17]\n",
      " [26]\n",
      " [55]]\n"
     ]
    }
   ],
   "source": [
    "# convert python list into numpy array\n",
    "testing_set = np.array(two_dim_lotto_array_test, ndmin=2)\n",
    "\n",
    "print (type(testing_set))\n",
    "print (testing_set.ndim)\n",
    "print (testing_set.shape)\n",
    "print (testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_actual.shape (9762,)\n",
      "y_actual [ 1 15 24 ..., 17 26 55]\n"
     ]
    }
   ],
   "source": [
    "y_actual = []\n",
    "for i in range(6, 9768):\n",
    "    y_actual.append(testing_set[i, 0])\n",
    "\n",
    "y_actual = np.array(y_actual)\n",
    "\n",
    "print (\"y_actual.shape\", y_actual.shape)\n",
    "print (\"y_actual\", y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape (9768, 1)\n",
      "inputs [[ 8]\n",
      " [13]\n",
      " [26]\n",
      " ..., \n",
      " [17]\n",
      " [26]\n",
      " [55]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "sc = RobustScaler()\n",
    "scaled_testing_set = sc.fit_transform(testing_set)\n",
    "scaled_testing_set = testing_set\n",
    "\n",
    "print (\"inputs.shape\", scaled_testing_set.shape)\n",
    "print (\"inputs\", scaled_testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape (9762, 6)\n",
      "inputs [[ 8 13 26 35 45 51]\n",
      " [13 26 35 45 51  1]\n",
      " [26 35 45 51  1 15]\n",
      " ..., \n",
      " [50 53 57  4  7 12]\n",
      " [53 57  4  7 12 17]\n",
      " [57  4  7 12 17 26]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 60 timesteps and t+1 output\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for i in range(6, 9768):\n",
    "    inputs.append(scaled_testing_set[i-6:i, 0])\n",
    "\n",
    "inputs, y_actual = np.array(inputs), np.array(y_actual)\n",
    "\n",
    "print (\"inputs.shape\", inputs.shape)\n",
    "print (\"inputs\", inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inputs.shape (9762, 6, 1)\n",
      "test_inputs [[[ 8]\n",
      "  [13]\n",
      "  [26]\n",
      "  [35]\n",
      "  [45]\n",
      "  [51]]\n",
      "\n",
      " [[13]\n",
      "  [26]\n",
      "  [35]\n",
      "  [45]\n",
      "  [51]\n",
      "  [ 1]]\n",
      "\n",
      " [[26]\n",
      "  [35]\n",
      "  [45]\n",
      "  [51]\n",
      "  [ 1]\n",
      "  [15]]\n",
      "\n",
      " ..., \n",
      " [[50]\n",
      "  [53]\n",
      "  [57]\n",
      "  [ 4]\n",
      "  [ 7]\n",
      "  [12]]\n",
      "\n",
      " [[53]\n",
      "  [57]\n",
      "  [ 4]\n",
      "  [ 7]\n",
      "  [12]\n",
      "  [17]]\n",
      "\n",
      " [[57]\n",
      "  [ 4]\n",
      "  [ 7]\n",
      "  [12]\n",
      "  [17]\n",
      "  [26]]]\n"
     ]
    }
   ],
   "source": [
    "# reshape input to fit keras predicted model format of 3-d\n",
    "test_inputs = np.reshape(inputs, (inputs.shape[0], inputs.shape[1],1))\n",
    "\n",
    "print (\"test_inputs.shape\", test_inputs.shape)\n",
    "print (\"test_inputs\", test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9762, 1)\n",
      "[[-0.9563427 ]\n",
      " [-2.63184738]\n",
      " [-0.1213398 ]\n",
      " ..., \n",
      " [ 0.19369698]\n",
      " [ 1.05630112]\n",
      " [ 0.07197189]]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's predict\n",
    "predicted_lotto_numbers = model.predict(test_inputs)\n",
    "\n",
    "# convert prediction back to normal values \n",
    "predicted_lotto_numbers = sc.inverse_transform(predicted_lotto_numbers)\n",
    "\n",
    "print (type(predicted_lotto_numbers))\n",
    "print (predicted_lotto_numbers.ndim)\n",
    "print (predicted_lotto_numbers.shape)\n",
    "print (predicted_lotto_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1\n",
      "(9762,)\n",
      "[-1. -3. -0. ...,  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# same for X_test\n",
    "y_predicted = np.around(predicted_lotto_numbers)\n",
    "y_predicted = np.reshape(y_predicted, (inputs.shape[0]))\n",
    "print (type(y_predicted))\n",
    "print (y_predicted.ndim)\n",
    "print (y_predicted.shape)\n",
    "print (y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1\n",
      "(9762,)\n",
      "[ 1 15 24 ..., 17 26 55]\n"
     ]
    }
   ],
   "source": [
    "print (type(y_actual))\n",
    "print (y_actual.ndim)\n",
    "print (y_actual.shape)\n",
    "print (y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ..., False False False]\n",
      "Accuracy: 0.000512190124974\n"
     ]
    }
   ],
   "source": [
    "true_predictions = np.equal(y_predicted, y_actual)\n",
    "print (true_predictions)\n",
    "print (\"Accuracy:\", np.sum(true_predictions)/np.size(true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 4: Let's Visualize the results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEWCAYAAACNJFuYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY0wYKxyMMcaE+X9+QbbbFceUDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12cf6efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_actual, color='red', label = 'Real Lotto Numbers')\n",
    "plt.plot(y_predicted, color='blue', label = 'Predicted Lotto Numbers')\n",
    "plt.title('Lotto Numbers Prediction')\n",
    "plt.xlabel('Lotto Results')\n",
    "plt.ylabel('Lotto Numbers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_lotto_numbers</th>\n",
       "      <th>sample_lotto_numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>39</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>54</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>59</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>38</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>47</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>50</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>21</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>44</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>56</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>29</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>32</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>34</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>37</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>52</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>57</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>15</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>26</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>51</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>55</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>57</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>12</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>32</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>37</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>38</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>50</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>6</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>13</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>31</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>33</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>39</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>28</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>30</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>37</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>49</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      real_lotto_numbers  sample_lotto_numbers\n",
       "0                      1                  -1.0\n",
       "1                     15                  -3.0\n",
       "2                     24                  -0.0\n",
       "3                     31                   6.0\n",
       "4                     34                   6.0\n",
       "5                     44                   6.0\n",
       "6                      3                  -1.0\n",
       "7                      8                  -1.0\n",
       "8                     29                  -2.0\n",
       "9                     30                   2.0\n",
       "10                    31                   3.0\n",
       "11                    49                   1.0\n",
       "12                    21                  -1.0\n",
       "13                    25                  -1.0\n",
       "14                    39                  -1.0\n",
       "15                    50                  -1.0\n",
       "16                    54                  -1.0\n",
       "17                    59                  -1.0\n",
       "18                    15                  -1.0\n",
       "19                    19                  -1.0\n",
       "20                    32                  -1.0\n",
       "21                    38                  -1.0\n",
       "22                    47                  -1.0\n",
       "23                    50                  -1.0\n",
       "24                    21                  -1.0\n",
       "25                    25                  -1.0\n",
       "26                    27                  -1.0\n",
       "27                    39                  -1.0\n",
       "28                    44                  -1.0\n",
       "29                    56                  -1.0\n",
       "...                  ...                   ...\n",
       "1170                  29                  -1.0\n",
       "1171                  32                  -1.0\n",
       "1172                  34                  -1.0\n",
       "1173                  37                  -1.0\n",
       "1174                  52                  -1.0\n",
       "1175                  57                  -1.0\n",
       "1176                   5                  -1.0\n",
       "1177                  15                  -1.0\n",
       "1178                  26                  -1.0\n",
       "1179                  51                  -0.0\n",
       "1180                  55                  -0.0\n",
       "1181                  57                  -1.0\n",
       "1182                   7                  -1.0\n",
       "1183                  12                  -1.0\n",
       "1184                  32                  -1.0\n",
       "1185                  37                  -1.0\n",
       "1186                  38                  -1.0\n",
       "1187                  50                  -1.0\n",
       "1188                   6                  -1.0\n",
       "1189                  10                  -1.0\n",
       "1190                  13                  -1.0\n",
       "1191                  31                  -1.0\n",
       "1192                  33                  -1.0\n",
       "1193                  39                  -1.0\n",
       "1194                   3                  -1.0\n",
       "1195                  28                  -1.0\n",
       "1196                  30                  -2.0\n",
       "1197                  37                   2.0\n",
       "1198                  49                   3.0\n",
       "1199                  54                   1.0\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_shape = real_lotto_numbers.reshape((11396))\n",
    "# print (type(new_shape))\n",
    "# print (new_shape.ndim)\n",
    "# print (new_shape.shape)\n",
    "# print (new_shape)\n",
    "                           \n",
    "real_lotto_numbers_df = pd.Series(y_actual.reshape((9762)), name='real_lotto_numbers')\n",
    "sample_lotto_numbers_df = pd.Series(np.around(y_predicted.reshape((9762))), name='sample_lotto_numbers')\n",
    "comparison_df = pd.concat([real_lotto_numbers_df, sample_lotto_numbers_df], axis=1)\n",
    "comparison_df.head(1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_lotto_numbers</th>\n",
       "      <th>sample_lotto_numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8562</th>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8563</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564</th>\n",
       "      <td>18</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565</th>\n",
       "      <td>36</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8566</th>\n",
       "      <td>48</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8567</th>\n",
       "      <td>53</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>15</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8569</th>\n",
       "      <td>16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8570</th>\n",
       "      <td>17</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>26</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8572</th>\n",
       "      <td>49</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>55</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>9</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8576</th>\n",
       "      <td>21</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8577</th>\n",
       "      <td>25</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8578</th>\n",
       "      <td>45</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8579</th>\n",
       "      <td>54</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8580</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8581</th>\n",
       "      <td>6</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8582</th>\n",
       "      <td>13</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8583</th>\n",
       "      <td>27</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>31</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>58</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>22</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8587</th>\n",
       "      <td>23</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>28</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>39</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>51</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>52</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9732</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9733</th>\n",
       "      <td>4</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9734</th>\n",
       "      <td>16</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>25</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>29</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>56</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>15</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>21</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>23</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9742</th>\n",
       "      <td>41</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>50</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9745</th>\n",
       "      <td>15</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9746</th>\n",
       "      <td>24</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9747</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9748</th>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751</th>\n",
       "      <td>10</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9752</th>\n",
       "      <td>38</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9753</th>\n",
       "      <td>50</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9754</th>\n",
       "      <td>53</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>57</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9756</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>12</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9759</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9760</th>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      real_lotto_numbers  sample_lotto_numbers\n",
       "8562                   5                  -1.0\n",
       "8563                   8                  -1.0\n",
       "8564                  18                  -1.0\n",
       "8565                  36                  -0.0\n",
       "8566                  48                  -0.0\n",
       "8567                  53                  -1.0\n",
       "8568                  15                  -1.0\n",
       "8569                  16                  -1.0\n",
       "8570                  17                  -1.0\n",
       "8571                  26                  -1.0\n",
       "8572                  49                  -1.0\n",
       "8573                  55                  -1.0\n",
       "8574                   2                  -1.0\n",
       "8575                   9                  -2.0\n",
       "8576                  21                  -2.0\n",
       "8577                  25                   3.0\n",
       "8578                  45                   5.0\n",
       "8579                  54                   3.0\n",
       "8580                   3                  -1.0\n",
       "8581                   6                  -1.0\n",
       "8582                  13                  -2.0\n",
       "8583                  27                   2.0\n",
       "8584                  31                   4.0\n",
       "8585                  58                   2.0\n",
       "8586                  22                  -1.0\n",
       "8587                  23                  -1.0\n",
       "8588                  28                  -1.0\n",
       "8589                  39                  -1.0\n",
       "8590                  51                  -1.0\n",
       "8591                  52                  -1.0\n",
       "...                  ...                   ...\n",
       "9732                   1                  -1.0\n",
       "9733                   4                  -3.0\n",
       "9734                  16                  -0.0\n",
       "9735                  25                   6.0\n",
       "9736                  29                   7.0\n",
       "9737                  56                   7.0\n",
       "9738                  14                   1.0\n",
       "9739                  15                  -1.0\n",
       "9740                  21                  -1.0\n",
       "9741                  23                  -1.0\n",
       "9742                  41                  -1.0\n",
       "9743                  50                  -1.0\n",
       "9744                   4                  -1.0\n",
       "9745                  15                  -1.0\n",
       "9746                  24                  -2.0\n",
       "9747                  33                   0.0\n",
       "9748                  37                   1.0\n",
       "9749                  58                   0.0\n",
       "9750                   2                  -1.0\n",
       "9751                  10                  -2.0\n",
       "9752                  38                  -2.0\n",
       "9753                  50                   3.0\n",
       "9754                  53                   5.0\n",
       "9755                  57                   3.0\n",
       "9756                   4                  -1.0\n",
       "9757                   7                  -1.0\n",
       "9758                  12                  -2.0\n",
       "9759                  17                   0.0\n",
       "9760                  26                   1.0\n",
       "9761                  55                   0.0\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df.tail(1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
