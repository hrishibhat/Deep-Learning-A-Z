{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import usual suspects\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>12/23/2016</td>\n",
       "      <td>790.90</td>\n",
       "      <td>792.74</td>\n",
       "      <td>787.28</td>\n",
       "      <td>789.91</td>\n",
       "      <td>623,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>12/27/2016</td>\n",
       "      <td>790.68</td>\n",
       "      <td>797.86</td>\n",
       "      <td>787.66</td>\n",
       "      <td>791.55</td>\n",
       "      <td>789,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>12/28/2016</td>\n",
       "      <td>793.70</td>\n",
       "      <td>794.23</td>\n",
       "      <td>783.20</td>\n",
       "      <td>785.05</td>\n",
       "      <td>1,153,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>12/29/2016</td>\n",
       "      <td>783.33</td>\n",
       "      <td>785.93</td>\n",
       "      <td>778.92</td>\n",
       "      <td>782.79</td>\n",
       "      <td>744,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>12/30/2016</td>\n",
       "      <td>782.75</td>\n",
       "      <td>782.78</td>\n",
       "      <td>770.41</td>\n",
       "      <td>771.82</td>\n",
       "      <td>1,770,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close     Volume\n",
       "1253  12/23/2016  790.90  792.74  787.28  789.91    623,400\n",
       "1254  12/27/2016  790.68  797.86  787.66  791.55    789,100\n",
       "1255  12/28/2016  793.70  794.23  783.20  785.05  1,153,800\n",
       "1256  12/29/2016  783.33  785.93  778.92  782.79    744,300\n",
       "1257  12/30/2016  782.75  782.78  770.41  771.82  1,770,000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing  the training set\n",
    "pd_training_set = pd.read_csv('Google_Stock_Price_Train.csv')\n",
    "pd_training_set.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(1258, 1)\n",
      "[[ 325.25]\n",
      " [ 331.27]\n",
      " [ 329.83]\n",
      " ..., \n",
      " [ 793.7 ]\n",
      " [ 783.33]\n",
      " [ 782.75]]\n"
     ]
    }
   ],
   "source": [
    "# NOw let's take only open price of stock\n",
    "# if we choose pd_training_set.iloc[:,1].values It will be only 1 dim numpy array\n",
    "# however, we need 2 dimension numpy array\n",
    "training_set_val = pd_training_set.iloc[:,1:2].values\n",
    "print (type(training_set_val))\n",
    "print (training_set_val.ndim)\n",
    "print (training_set_val.shape)\n",
    "print (training_set_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08581368]\n",
      " [ 0.09701243]\n",
      " [ 0.09433366]\n",
      " ..., \n",
      " [ 0.95725128]\n",
      " [ 0.93796041]\n",
      " [ 0.93688146]]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's do normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "training_set = sc.fit_transform(training_set_val)\n",
    "print (training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08581368]\n",
      " [ 0.09701243]\n",
      " [ 0.09433366]\n",
      " ..., \n",
      " [ 0.95163331]\n",
      " [ 0.95725128]\n",
      " [ 0.93796041]]\n"
     ]
    }
   ],
   "source": [
    "# setting X_train and y_train\n",
    "X_train = training_set[0:1257]\n",
    "print (X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09701243]\n",
      " [ 0.09433366]\n",
      " [ 0.09156187]\n",
      " ..., \n",
      " [ 0.95725128]\n",
      " [ 0.93796041]\n",
      " [ 0.93688146]]\n"
     ]
    }
   ],
   "source": [
    "y_train = training_set[1:1258]\n",
    "print (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.08581368]]\n",
      "\n",
      " [[ 0.09701243]]\n",
      "\n",
      " [[ 0.09433366]]\n",
      "\n",
      " ..., \n",
      " [[ 0.95163331]]\n",
      "\n",
      " [[ 0.95725128]]\n",
      "\n",
      " [[ 0.93796041]]]\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "\n",
    "# chaging from 2 to 3 dimension array, by addding time step as 3rd dimension\n",
    "# corresponds to (batch_size, timesteps, input_dim)\n",
    "X_train = np.reshape(X_train,(1257,1,1))\n",
    "print (X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_train = np.reshape(y_train)\n",
    "# print (y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Initializing the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the Keras liabraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regressor bassed upon sequential RNN\n",
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units=4, activation='sigmoid', input_shape=(None, 1)))\n",
    "regressor.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let' compile our RNN regressor\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.7001     \n",
      "Epoch 2/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.5670     \n",
      "Epoch 3/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.4608     \n",
      "Epoch 4/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.3762     \n",
      "Epoch 5/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.3085     \n",
      "Epoch 6/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.2549     \n",
      "Epoch 7/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.2127     \n",
      "Epoch 8/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.1795     \n",
      "Epoch 9/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.1535     \n",
      "Epoch 10/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.1333     \n",
      "Epoch 11/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.1179     \n",
      "Epoch 12/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.1063     \n",
      "Epoch 13/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0978     \n",
      "Epoch 14/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0916     \n",
      "Epoch 15/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0872     \n",
      "Epoch 16/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0841     \n",
      "Epoch 17/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0820     \n",
      "Epoch 18/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0804     \n",
      "Epoch 19/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0793     \n",
      "Epoch 20/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0784     \n",
      "Epoch 21/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0778     \n",
      "Epoch 22/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0773     \n",
      "Epoch 23/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0769     \n",
      "Epoch 24/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0764     \n",
      "Epoch 25/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0760     \n",
      "Epoch 26/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0756     \n",
      "Epoch 27/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0752     \n",
      "Epoch 28/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0747     \n",
      "Epoch 29/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0742     \n",
      "Epoch 30/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0738     \n",
      "Epoch 31/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0732     \n",
      "Epoch 32/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0727     \n",
      "Epoch 33/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0721     \n",
      "Epoch 34/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0714     \n",
      "Epoch 35/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0708     \n",
      "Epoch 36/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0701     \n",
      "Epoch 37/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0693     \n",
      "Epoch 38/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0686     \n",
      "Epoch 39/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0677     \n",
      "Epoch 40/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0669     \n",
      "Epoch 41/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0659     \n",
      "Epoch 42/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0649     \n",
      "Epoch 43/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0639     \n",
      "Epoch 44/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0628     - ETA: 0s - loss: 0.0\n",
      "Epoch 45/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0617     \n",
      "Epoch 46/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0604     \n",
      "Epoch 47/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0592     \n",
      "Epoch 48/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0579     \n",
      "Epoch 49/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0565     \n",
      "Epoch 50/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0550     \n",
      "Epoch 51/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0536     \n",
      "Epoch 52/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0521     \n",
      "Epoch 53/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0506     \n",
      "Epoch 54/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0490     \n",
      "Epoch 55/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0474     \n",
      "Epoch 56/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0457     \n",
      "Epoch 57/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0439     \n",
      "Epoch 58/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0422     \n",
      "Epoch 59/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0404     \n",
      "Epoch 60/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0386     \n",
      "Epoch 61/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0367     \n",
      "Epoch 62/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0348     \n",
      "Epoch 63/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0329     \n",
      "Epoch 64/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0310     \n",
      "Epoch 65/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0291     \n",
      "Epoch 66/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0272     \n",
      "Epoch 67/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0254     \n",
      "Epoch 68/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0236     \n",
      "Epoch 69/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0219     \n",
      "Epoch 70/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0202     \n",
      "Epoch 71/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0186     \n",
      "Epoch 72/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0171     \n",
      "Epoch 73/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0156     \n",
      "Epoch 74/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0142     \n",
      "Epoch 75/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0128     \n",
      "Epoch 76/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0115     \n",
      "Epoch 77/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0103     \n",
      "Epoch 78/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0092     \n",
      "Epoch 79/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0082     \n",
      "Epoch 80/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 81/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0063     \n",
      "Epoch 82/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0055     \n",
      "Epoch 83/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0048     \n",
      "Epoch 84/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0041     \n",
      "Epoch 85/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0036     \n",
      "Epoch 86/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0030     \n",
      "Epoch 87/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0026     \n",
      "Epoch 88/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0022     \n",
      "Epoch 89/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0019     \n",
      "Epoch 90/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0016     \n",
      "Epoch 91/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0013     \n",
      "Epoch 92/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0011     \n",
      "Epoch 93/200\n",
      "1257/1257 [==============================] - 0s - loss: 9.6997e-04     \n",
      "Epoch 94/200\n",
      "1257/1257 [==============================] - 0s - loss: 8.2925e-04     \n",
      "Epoch 95/200\n",
      "1257/1257 [==============================] - 0s - loss: 7.1423e-04     \n",
      "Epoch 96/200\n",
      "1257/1257 [==============================] - 0s - loss: 6.1992e-04     \n",
      "Epoch 97/200\n",
      "1257/1257 [==============================] - 0s - loss: 5.4271e-04     \n",
      "Epoch 98/200\n",
      "1257/1257 [==============================] - 0s - loss: 4.8293e-04     \n",
      "Epoch 99/200\n",
      "1257/1257 [==============================] - 0s - loss: 4.3360e-04     \n",
      "Epoch 100/200\n",
      "1257/1257 [==============================] - 0s - loss: 3.9659e-04     \n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257/1257 [==============================] - 0s - loss: 3.6637e-04     \n",
      "Epoch 102/200\n",
      "1257/1257 [==============================] - 0s - loss: 3.4320e-04     \n",
      "Epoch 103/200\n",
      "1257/1257 [==============================] - 0s - loss: 3.2585e-04     \n",
      "Epoch 104/200\n",
      "1257/1257 [==============================] - 0s - loss: 3.1135e-04     \n",
      "Epoch 105/200\n",
      "1257/1257 [==============================] - 0s - loss: 3.0104e-04     \n",
      "Epoch 106/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.9300e-04     \n",
      "Epoch 107/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.8752e-04     \n",
      "Epoch 108/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.8239e-04     \n",
      "Epoch 109/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.7885e-04     \n",
      "Epoch 110/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.7669e-04     \n",
      "Epoch 111/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.7403e-04     \n",
      "Epoch 112/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.7202e-04     \n",
      "Epoch 113/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.7145e-04     \n",
      "Epoch 114/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6876e-04     \n",
      "Epoch 115/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6835e-04     \n",
      "Epoch 116/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6786e-04     \n",
      "Epoch 117/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6650e-04     \n",
      "Epoch 118/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6556e-04     \n",
      "Epoch 119/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6527e-04     \n",
      "Epoch 120/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6458e-04     \n",
      "Epoch 121/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6402e-04     \n",
      "Epoch 122/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6368e-04     \n",
      "Epoch 123/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6283e-04     \n",
      "Epoch 124/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6279e-04     \n",
      "Epoch 125/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6227e-04     \n",
      "Epoch 126/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6168e-04     \n",
      "Epoch 127/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6139e-04     \n",
      "Epoch 128/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6139e-04     \n",
      "Epoch 129/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6059e-04     \n",
      "Epoch 130/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6091e-04     \n",
      "Epoch 131/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6005e-04     \n",
      "Epoch 132/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5964e-04     \n",
      "Epoch 133/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5950e-04     \n",
      "Epoch 134/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5945e-04     \n",
      "Epoch 135/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5959e-04     \n",
      "Epoch 136/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5869e-04     \n",
      "Epoch 137/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5879e-04     \n",
      "Epoch 138/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5825e-04     \n",
      "Epoch 139/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5813e-04     \n",
      "Epoch 140/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5762e-04     \n",
      "Epoch 141/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5792e-04     \n",
      "Epoch 142/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5773e-04     \n",
      "Epoch 143/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5701e-04     \n",
      "Epoch 144/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5757e-04     \n",
      "Epoch 145/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5679e-04     \n",
      "Epoch 146/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5623e-04     \n",
      "Epoch 147/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5569e-04     \n",
      "Epoch 148/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5611e-04     \n",
      "Epoch 149/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5606e-04     \n",
      "Epoch 150/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5624e-04     \n",
      "Epoch 151/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5572e-04     \n",
      "Epoch 152/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5539e-04     \n",
      "Epoch 153/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5550e-04     \n",
      "Epoch 154/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5508e-04     \n",
      "Epoch 155/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5642e-04     \n",
      "Epoch 156/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5474e-04     \n",
      "Epoch 157/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5448e-04     \n",
      "Epoch 158/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5548e-04     \n",
      "Epoch 159/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5462e-04     \n",
      "Epoch 160/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5416e-04     \n",
      "Epoch 161/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5412e-04     \n",
      "Epoch 162/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5419e-04     \n",
      "Epoch 163/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5370e-04     \n",
      "Epoch 164/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5404e-04     \n",
      "Epoch 165/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5384e-04     \n",
      "Epoch 166/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5373e-04     \n",
      "Epoch 167/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5401e-04     \n",
      "Epoch 168/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5313e-04     \n",
      "Epoch 169/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5268e-04     \n",
      "Epoch 170/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5300e-04     \n",
      "Epoch 171/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5369e-04     \n",
      "Epoch 172/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5286e-04     \n",
      "Epoch 173/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5307e-04     \n",
      "Epoch 174/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5154e-04     \n",
      "Epoch 175/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5506e-04     \n",
      "Epoch 176/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5236e-04     \n",
      "Epoch 177/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5228e-04     \n",
      "Epoch 178/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5237e-04     \n",
      "Epoch 179/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5326e-04     \n",
      "Epoch 180/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5246e-04     \n",
      "Epoch 181/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5187e-04     \n",
      "Epoch 182/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5215e-04     \n",
      "Epoch 183/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5332e-04     \n",
      "Epoch 184/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5296e-04     \n",
      "Epoch 185/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5192e-04     \n",
      "Epoch 186/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5152e-04     \n",
      "Epoch 187/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5319e-04     \n",
      "Epoch 188/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5258e-04     \n",
      "Epoch 189/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5225e-04     \n",
      "Epoch 190/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5509e-04     \n",
      "Epoch 191/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5563e-04     \n",
      "Epoch 192/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5199e-04     \n",
      "Epoch 193/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5353e-04     \n",
      "Epoch 194/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5367e-04     \n",
      "Epoch 195/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5471e-04     \n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257/1257 [==============================] - 0s - loss: 2.5113e-04     \n",
      "Epoch 197/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5210e-04     \n",
      "Epoch 198/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5321e-04     \n",
      "Epoch 199/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5205e-04     \n",
      "Epoch 200/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5193e-04     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c14d1d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train ,y_train, batch_size=32, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Making Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1/26/2017</td>\n",
       "      <td>837.81</td>\n",
       "      <td>838.00</td>\n",
       "      <td>827.01</td>\n",
       "      <td>832.15</td>\n",
       "      <td>2,973,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1/27/2017</td>\n",
       "      <td>834.71</td>\n",
       "      <td>841.95</td>\n",
       "      <td>820.44</td>\n",
       "      <td>823.31</td>\n",
       "      <td>2,965,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1/30/2017</td>\n",
       "      <td>814.66</td>\n",
       "      <td>815.84</td>\n",
       "      <td>799.80</td>\n",
       "      <td>802.32</td>\n",
       "      <td>3,246,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1/31/2017</td>\n",
       "      <td>796.86</td>\n",
       "      <td>801.25</td>\n",
       "      <td>790.52</td>\n",
       "      <td>796.79</td>\n",
       "      <td>2,160,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2/1/2017</td>\n",
       "      <td>799.68</td>\n",
       "      <td>801.19</td>\n",
       "      <td>791.19</td>\n",
       "      <td>795.70</td>\n",
       "      <td>2,029,700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open    High     Low   Close     Volume\n",
       "16  1/26/2017  837.81  838.00  827.01  832.15  2,973,900\n",
       "17  1/27/2017  834.71  841.95  820.44  823.31  2,965,800\n",
       "18  1/30/2017  814.66  815.84  799.80  802.32  3,246,600\n",
       "19  1/31/2017  796.86  801.25  790.52  796.79  2,160,600\n",
       "20   2/1/2017  799.68  801.19  791.19  795.70  2,029,700"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing for Test_set\n",
    "# Importing the test set\n",
    "pd_testing_set = pd.read_csv('Google_Stock_Price_Test.csv')\n",
    "pd_testing_set.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "2\n",
      "(21, 1)\n",
      "      Open\n",
      "0   778.81\n",
      "1   788.36\n",
      "2   786.08\n",
      "3   795.26\n",
      "4   806.40\n",
      "5   807.86\n",
      "6   805.00\n",
      "7   807.14\n",
      "8   807.48\n",
      "9   807.08\n",
      "10  805.81\n",
      "11  805.12\n",
      "12  806.91\n",
      "13  807.25\n",
      "14  822.30\n",
      "15  829.62\n",
      "16  837.81\n",
      "17  834.71\n",
      "18  814.66\n",
      "19  796.86\n",
      "20  799.68\n"
     ]
    }
   ],
   "source": [
    "# NOw let's take only open price of stock\n",
    "# if we choose pd_training_set.iloc[:,1] It will be only 1 dim pandas series\n",
    "# however, we need 2 dimension, so\n",
    "real_stock_price = pd_testing_set.iloc[:,1:2]\n",
    "print (type(real_stock_price))\n",
    "print (real_stock_price.ndim)\n",
    "print (real_stock_price.shape)\n",
    "print (real_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.16186441]\n",
      " [ 0.12322034]\n",
      " [ 0.27881356]\n",
      " [ 0.46762712]\n",
      " [ 0.49237288]\n",
      " [ 0.44389831]\n",
      " [ 0.48016949]\n",
      " [ 0.4859322 ]\n",
      " [ 0.47915254]\n",
      " [ 0.45762712]\n",
      " [ 0.4459322 ]\n",
      " [ 0.47627119]\n",
      " [ 0.4820339 ]\n",
      " [ 0.73711864]\n",
      " [ 0.86118644]\n",
      " [ 1.        ]\n",
      " [ 0.94745763]\n",
      " [ 0.60762712]\n",
      " [ 0.3059322 ]\n",
      " [ 0.35372881]]\n"
     ]
    }
   ],
   "source": [
    "testing_set = sc.fit_transform(real_stock_price)\n",
    "print (testing_set)\n",
    "# in the end inverse fit transform to get normal stock open price back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.16186441]\n",
      " [ 0.12322034]\n",
      " [ 0.27881356]\n",
      " [ 0.46762712]\n",
      " [ 0.49237288]\n",
      " [ 0.44389831]\n",
      " [ 0.48016949]\n",
      " [ 0.4859322 ]\n",
      " [ 0.47915254]\n",
      " [ 0.45762712]\n",
      " [ 0.4459322 ]\n",
      " [ 0.47627119]\n",
      " [ 0.4820339 ]\n",
      " [ 0.73711864]\n",
      " [ 0.86118644]\n",
      " [ 1.        ]\n",
      " [ 0.94745763]\n",
      " [ 0.60762712]\n",
      " [ 0.3059322 ]]\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(20, 1)\n",
      "[[ 0.        ]\n",
      " [ 0.16186441]\n",
      " [ 0.12322034]\n",
      " [ 0.27881356]\n",
      " [ 0.46762712]\n",
      " [ 0.49237288]\n",
      " [ 0.44389831]\n",
      " [ 0.48016949]\n",
      " [ 0.4859322 ]\n",
      " [ 0.47915254]\n",
      " [ 0.45762712]\n",
      " [ 0.4459322 ]\n",
      " [ 0.47627119]\n",
      " [ 0.4820339 ]\n",
      " [ 0.73711864]\n",
      " [ 0.86118644]\n",
      " [ 1.        ]\n",
      " [ 0.94745763]\n",
      " [ 0.60762712]\n",
      " [ 0.3059322 ]]\n"
     ]
    }
   ],
   "source": [
    "# same for X_test\n",
    "X_test = testing_set[0:20]\n",
    "print (X_test)\n",
    "print (type(X_test))\n",
    "print (X_test.ndim)\n",
    "print (X_test.shape)\n",
    "print (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.        ]]\n",
      "\n",
      " [[ 0.16186441]]\n",
      "\n",
      " [[ 0.12322034]]\n",
      "\n",
      " [[ 0.27881356]]\n",
      "\n",
      " [[ 0.46762712]]\n",
      "\n",
      " [[ 0.49237288]]\n",
      "\n",
      " [[ 0.44389831]]\n",
      "\n",
      " [[ 0.48016949]]\n",
      "\n",
      " [[ 0.4859322 ]]\n",
      "\n",
      " [[ 0.47915254]]\n",
      "\n",
      " [[ 0.45762712]]\n",
      "\n",
      " [[ 0.4459322 ]]\n",
      "\n",
      " [[ 0.47627119]]\n",
      "\n",
      " [[ 0.4820339 ]]\n",
      "\n",
      " [[ 0.73711864]]\n",
      "\n",
      " [[ 0.86118644]]\n",
      "\n",
      " [[ 1.        ]]\n",
      "\n",
      " [[ 0.94745763]]\n",
      "\n",
      " [[ 0.60762712]]\n",
      "\n",
      " [[ 0.3059322 ]]]\n",
      "<class 'numpy.ndarray'>\n",
      "3\n",
      "(20, 1, 1)\n",
      "[[[ 0.        ]]\n",
      "\n",
      " [[ 0.16186441]]\n",
      "\n",
      " [[ 0.12322034]]\n",
      "\n",
      " [[ 0.27881356]]\n",
      "\n",
      " [[ 0.46762712]]\n",
      "\n",
      " [[ 0.49237288]]\n",
      "\n",
      " [[ 0.44389831]]\n",
      "\n",
      " [[ 0.48016949]]\n",
      "\n",
      " [[ 0.4859322 ]]\n",
      "\n",
      " [[ 0.47915254]]\n",
      "\n",
      " [[ 0.45762712]]\n",
      "\n",
      " [[ 0.4459322 ]]\n",
      "\n",
      " [[ 0.47627119]]\n",
      "\n",
      " [[ 0.4820339 ]]\n",
      "\n",
      " [[ 0.73711864]]\n",
      "\n",
      " [[ 0.86118644]]\n",
      "\n",
      " [[ 1.        ]]\n",
      "\n",
      " [[ 0.94745763]]\n",
      "\n",
      " [[ 0.60762712]]\n",
      "\n",
      " [[ 0.3059322 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "\n",
    "# chaging from 2 to 3 dimension array, by addding time step as 3rd dimension\n",
    "# corresponds to (batch_size, timesteps, input_dim)\n",
    "test_inputs = np.reshape(X_test,(20,1,1))\n",
    "print (test_inputs)\n",
    "print (type(test_inputs))\n",
    "print (test_inputs.ndim)\n",
    "print (test_inputs.shape)\n",
    "print (test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 779.16113281]\n",
      " [ 788.31689453]\n",
      " [ 786.10021973]\n",
      " [ 795.13104248]\n",
      " [ 806.38348389]\n",
      " [ 807.8727417 ]\n",
      " [ 804.9576416 ]\n",
      " [ 807.13800049]\n",
      " [ 807.48492432]\n",
      " [ 807.07684326]\n",
      " [ 805.7822876 ]\n",
      " [ 805.07977295]\n",
      " [ 806.90344238]\n",
      " [ 807.25030518]\n",
      " [ 822.65966797]\n",
      " [ 829.68115234]\n",
      " [ 836.31219482]\n",
      " [ 834.29217529]\n",
      " [ 814.83007812]\n",
      " [ 796.73114014]]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's predict\n",
    "y_test = regressor.predict(test_inputs)\n",
    "\n",
    "# inverse scaled values to get real stock price\n",
    "predicted_stock_price = sc.inverse_transform(y_test)\n",
    "print (predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 4: Let's Visualize the results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQmlBCEiIqGUIEREJJQShIiIhPof4YjM8iIFZycAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c589cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(real_stock_price, color='red', label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color='blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>788.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>786.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>795.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>806.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open\n",
       "0  778.81\n",
       "1  788.36\n",
       "2  786.08\n",
       "3  795.26\n",
       "4  806.40"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price_df = pd.DataFrame(real_stock_price)\n",
    "print (type(real_stock_price_df))\n",
    "real_stock_price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted_Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>779.161133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>788.316895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>786.100220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>795.131042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>806.383484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted_Open\n",
       "0      779.161133\n",
       "1      788.316895\n",
       "2      786.100220\n",
       "3      795.131042\n",
       "4      806.383484"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_df = pd.DataFrame(predicted_stock_price, columns=['Predicted_Open'])\n",
    "print (type(predicted_stock_df))\n",
    "predicted_stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     778.81\n",
      "1     788.36\n",
      "2     786.08\n",
      "3     795.26\n",
      "4     806.40\n",
      "5     807.86\n",
      "6     805.00\n",
      "7     807.14\n",
      "8     807.48\n",
      "9     807.08\n",
      "10    805.81\n",
      "11    805.12\n",
      "12    806.91\n",
      "13    807.25\n",
      "14    822.30\n",
      "15    829.62\n",
      "16    837.81\n",
      "17    834.71\n",
      "18    814.66\n",
      "19    796.86\n",
      "20    799.68\n",
      "Name: Open, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print (pd_testing_set.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "2 columns passed, passed data had 21 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e610601dd265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomparsion_sheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd_testing_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted_stock_price\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Real_Open_Price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Predicted_Open_Price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomparsion_sheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    303\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   5539\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5540\u001b[0m         return _list_to_arrays(data, columns, coerce_float=coerce_float,\n\u001b[0;32m-> 5541\u001b[0;31m                                dtype=dtype)\n\u001b[0m\u001b[1;32m   5542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   5596\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5597\u001b[0m     return _convert_object_array(content, columns, dtype=dtype,\n\u001b[0;32m-> 5598\u001b[0;31m                                  coerce_float=coerce_float)\n\u001b[0m\u001b[1;32m   5599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_convert_object_array\u001b[0;34m(content, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   5655\u001b[0m             \u001b[0;31m# caller's responsibility to check for this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5656\u001b[0m             raise AssertionError('%d columns passed, passed data had %s '\n\u001b[0;32m-> 5657\u001b[0;31m                                  'columns' % (len(columns), len(content)))\n\u001b[0m\u001b[1;32m   5658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5659\u001b[0m     \u001b[0;31m# provide soft conversion of object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 2 columns passed, passed data had 21 columns"
     ]
    }
   ],
   "source": [
    "comparsion_sheet = pd.DataFrame([pd_testing_set.iloc[:,1].values,predicted_stock_price], columns=['Real_Open_Price','Predicted_Open_Price'])\n",
    "comparsion_sheet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
